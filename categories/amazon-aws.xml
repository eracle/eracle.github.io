<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Thoughts and Stuff (Posts about Amazon AWS)</title><link>http://eracle.github.io/</link><description></description><atom:link rel="self" href="http://eracle.github.io/categories/amazon-aws.xml" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2017 &lt;a href="mailto:eracle@posteo.eu"&gt;Antonio Ercole De Luca (eracle)&lt;/a&gt; </copyright><lastBuildDate>Fri, 15 Sep 2017 11:13:04 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Scraper on AWS Elastic Beanstalk - Don't do that!</title><link>http://eracle.github.io/posts/scraper-on-aws-elastic-beanstalk-dont-do-that.html</link><dc:creator>Antonio Ercole De Luca (eracle)</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;img alt="Amazon AWS - Autoscaling" src="http://docs.aws.amazon.com/autoscaling/latest/userguide/images/as-basic-diagram.png"&gt;&lt;/p&gt;
&lt;p&gt;During one of my past coding experiences, I had to modify and maintain an already started Django project.&lt;/p&gt;
&lt;p&gt;The project architecture was (already) designed to have a Django based REST APIs (with django-rest-framework) which used to run on Amazon Elastic Beanstalk. By not deepening too much on details, there was the core content of the database that was filled by a semi-automatic procedure. That procedure resided on the same project's code base, and involved the instantiation of a scraper module, which therefore, used to run on the same EC2 instance where the web server was. By designing it so, who developed it, had direct access to the database by using the Django's data access functionalities.
The issued scraper module was a &lt;a href="https://docs.djangoproject.com/en/1.11/howto/custom-management-commands/"&gt;django-admin command&lt;/a&gt; based on the python &lt;a href="http://docs.python-requests.org/en/master/"&gt;requests&lt;/a&gt; library, that used to parse the downloaded HTML files through the &lt;a href="https://packages.ubuntu.com/xenial/python-lxml"&gt;lxml&lt;/a&gt; library.&lt;/p&gt;
&lt;p&gt;This architecture had at least two inconveniences: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First, since the EC2 instances were not designed to have an high number of requests per second, they were not configured to have a big amount of RAM memory. So, without manually changing the EB configuration, after being activated, the scraping process was killed before reaching complete usage of its available memory by the &lt;a href="http://manpages.ubuntu.com/manpages/xenial/man8/watchdog.8.html"&gt;watchdog&lt;/a&gt; daemon of Ubuntu 16.04. 
In order to solve this problem, I had to manually change the EB configuration, by asking for a different instance type, for instance: t2.medium or t2.large.
This manual process creates a new EC2 instance with enhanced performances, and introduced a downtime period for the back-end. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Second, when the scraper run, slowed down the Django back-end, whom responses took longer time to be generated.
At that time the default configuration on the production environment had the Amazon Auto-scaling feature turned on, that feature, by continuously testing the response times of the instances, starts to create additional EC2 instances if the average response time raises too much. 
That used to happen multiple times during the execution of the Scraping process, since it was manually started on the same EC2 instance, and used to slow down the web server.
Let's naivelly close an eye on the fact that it was inefficient in terms of bills, the problem was that the Auto-scaling system after having increased the number of EC2 instances and having assessed that then the response time lowered, started the process of terminating some of them, in order to decrease their number, since it believes they are not needed anymore.
That termination used to happen in a random fashion, in other words, the terminated EC2 instances were randomly chosen, so, the first EC2 instance risked to be shutdown and to not finish the scraping process. 
As a possible solution, if the Amazon Auto-scaling feature would have been turned off, since on Amazon AWS the HTTPS configuration is strongly tightened with the Auto-scaling one, the production platform turned out to be not reachable via its normal URL, in other words: it was out of service for users.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After some troubles dealing with this architecture, I decided to solve the problem and migrate the scraper module to a different code base that leverages the &lt;a href="https://scrapy.org/"&gt;scrapy&lt;/a&gt; library and &lt;a href="https://scrapinghub.com/"&gt;scrapinghub&lt;/a&gt; platform, but this is another (interesting) story.&lt;/p&gt;
&lt;h4&gt;EDIT:&lt;/h4&gt;
&lt;p&gt;I just get aware that I could have configured autoscaling for &lt;a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/as-instance-termination.html#instance-protection-instance"&gt;instance protection&lt;/a&gt; to avoid the termination of the issued instance where the scraper ran. Well, in the end, by having moved the scraper out of the web server, and having modified the project toward a microservices architecture, in my opinion, still, was not a bad idea.&lt;/p&gt;&lt;/div&gt;</description><category>Amazon AWS</category><category>django</category><category>scrapy</category><guid>http://eracle.github.io/posts/scraper-on-aws-elastic-beanstalk-dont-do-that.html</guid><pubDate>Sat, 09 Sep 2017 07:05:10 GMT</pubDate></item></channel></rss>