<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Personal Page</title><link>http://eracle.github.io/</link><description>Antonio Ercole De Luca Page</description><atom:link type="application/rss+xml" href="http://eracle.github.io/rss.xml" rel="self"></atom:link><language>en</language><copyright>Contents Â© 2018 &lt;a href="mailto:eracle@posteo.eu"&gt;Antonio Ercole De Luca (eracle)&lt;/a&gt; </copyright><lastBuildDate>Tue, 19 Jun 2018 11:49:26 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Interview questions (and answers):</title><link>http://eracle.github.io/posts/pre-interview-questions.html</link><dc:creator>Antonio Ercole De Luca (eracle)</dc:creator><description>&lt;div&gt;&lt;p&gt;I want to share here some answers I have written, during a pre-interview that I've approached for the role of sr. backend software engineer:&lt;/p&gt;
&lt;h5&gt;How would you design an horizontally scalable REST-API? Which techniques? Which Backend Architecture? Database?&lt;/h5&gt;
&lt;p&gt;Also influenced by the tools I've already used, I would use Amazon AWS, in particular, for the Django web server: Elastic Beanstalk and its Auto Scaling feature, which automatically deploys new back-end instances during peak load periods. That would implement an horizontally scalable REST-API.
Regarding the DB, horizontal scaling means using a solution with a read-write instance and a pool of read-only replicas. As this &lt;a href="https://aws.amazon.com/blogs/database/scaling-your-amazon-rds-instance-vertically-and-horizontally/"&gt;Amazon RDS&lt;/a&gt; article points out, this solution can be easily implemented by using Amazon RDS, which allows to configure read-only database replicas continuously kept up-to-date, under the hood, in an asynchronous way by the service it selves. Regarding the single read-write instance, Amazon RDS allows Vertical Scaling by offering DB instance classes with different performances and costs, so the right DB class can be chosen if the expected load of write queries is forecastable. &lt;/p&gt;
&lt;h5&gt;How does Django supports backend scaling?&lt;/h5&gt;
&lt;p&gt;Django provides the Database &lt;a href="https://docs.djangoproject.com/en/1.11/topics/db/multi-db/#an-example"&gt;Routers configuration&lt;/a&gt;, which allows to define different database endpoints and different routing rules for each of them. If its performances are not satisfactory, with AWS, there is the possibility to configure an &lt;a href="http://docs.aws.amazon.com/opsworks/latest/userguide/layers-haproxy.html"&gt;HAProxy layer&lt;/a&gt; as an AWS OpsWorks Stacks Layer. In both cases, write queries would be routed to the read-write DB instance and read queries to the read-only replicas.&lt;/p&gt;
&lt;h5&gt;Please, give some details over CSRF protection provided by Django REST framework.&lt;/h5&gt;
&lt;p&gt;CSRF is a security concern that affects browsers or apps that contain an embedded browser, like an Android WebView.
There are different use cases for the REST APIs consumption:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stand-alone clients like Android apps or Desktop applications:
In this case, django-rest-framework's TokenAuthentication would be used, which does not run the CSRF security checks;&lt;/li&gt;
&lt;li&gt;AJAX calls of a client browser:
In this case, django-rest-framework's SessionAuthentication is the one used. It does run the CSRF security checks, so there are some API's design rules that have to be followed:
First, the REST APIs must have all GETs endpoints side effect free, that means, those endpoints do not have to change the DB's data, in other words, they are read-only toward the DB.
Second, for all the non GETs calls, each client must send a custom HTTP header, namelly 'X-CSRFToken', with the value of the CSRF token cookie the backend provides after authentication.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;How would you implement the execution of a long-running task (5 minutes) triggered by a POST message?&lt;/h5&gt;
&lt;p&gt;Obviously, is not possible to execute the long-running task during the request-response cycle. Http requests are neither designed nor suited for this kind of behavior, and a time-out is what would happen during such as an heretic implementation.&lt;/p&gt;
&lt;p&gt;As the one I have used on the &lt;a href="https://github.com/eracle/rbroker"&gt;rbroker&lt;/a&gt; source code, python offers a plenty of asynchronous libraries, in particular, I used django_q, which allows to define asynchronous tasks, which are executed by threads (workers) of a different process. &lt;/p&gt;
&lt;p&gt;Asynchronous libraries use to use brokers, which are data storages where the information needed by the workers are kept and shared.
For instance, I have configured django_q to use as broker the same underling database used by the others django apps. This is not efficient on production environments, since by competing on accessing the same database it would slow down the rest of the system.
Among the others python asynchronous libraries, worth to be noted, there is celery, which I had to deal with during one of my past working experiences. It offers a variety of different brokers, such as Redis, an efficient key-value storage, and Amazon SQS, a broker as-a-service offered by Amazon AWS. &lt;/p&gt;
&lt;h5&gt;How would you implement the send of an e-mail, through a known REST API, triggered by a POST message received, taking into account that the send may be not successful?&lt;/h5&gt;
&lt;p&gt;During one of my past working experiences, we had to send emails during the normal user's work-flow, for instance subscription confirms. The project code-base was using the &lt;a href="https://docs.djangoproject.com/en/1.11/topics/email/"&gt;django mail module&lt;/a&gt;. After some time, some emails were hidden in the spam buckets of our users' email service provider. After that, we decided to outsource the task to an external provider, at that time the decision was &lt;a href="https://www.mailgun.com/"&gt;mailgun&lt;/a&gt;. It allowed to free send up to 10,000 emails every month, in an asynchronous way, by using a REST API. In particular, by leveraging the &lt;a href="https://github.com/anymail/django-anymail"&gt;django-anymail&lt;/a&gt; library, which offers a unified python API toward different providers, such as: Mailgun, Mailjet, Postmark, SendGrid, SparkPost and others.
Regarding the question, I would use mailgun and its tracking functionality, which allows to track the status of the sending process. In particular, there is the possibility to send webhooks, which are endpoint's URLs where mailgun POSTs all the events regarding the submitted emails.
So, when a particular event regarding a single email is received, a decision can be taken, but in order to here suggest a proper solution 
 there must be taken into account other project constraints, such as: importance of the emails sent or which are the policies over wrong email addresses.&lt;/p&gt;&lt;/div&gt;</description><category>Amazon AWS</category><category>django</category><category>DRF</category><category>Interview</category><category>Mailgun</category><category>REST API</category><guid>http://eracle.github.io/posts/pre-interview-questions.html</guid><pubDate>Tue, 10 Oct 2017 07:05:10 GMT</pubDate></item><item><title>Scraper on AWS Elastic Beanstalk - Please don't do that!</title><link>http://eracle.github.io/posts/scraper-on-aws-elastic-beanstalk-dont-do-that.html</link><dc:creator>Antonio Ercole De Luca (eracle)</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;img alt="Amazon AWS - Autoscaling" src="http://docs.aws.amazon.com/autoscaling/latest/userguide/images/as-basic-diagram.png"&gt;&lt;/p&gt;
&lt;p&gt;During one of my past working experiences, I had to modify and maintain an already started Django project.&lt;/p&gt;
&lt;p&gt;The project architecture was designed to have a Django based REST APIs (with django-rest-framework) which used to run on Amazon Elastic Beanstalk. By not deepening too much on details, there was the core content of the database that was filled by a semi-automatic procedure. That procedure resided on the same project's code base, and involved the instantiation of a scraper module, which therefore, used to run on the same EC2 instance where the web server was. By designing it so, who developed it, had direct access to the database by using the Django's data access functionalities.
The issued scraper module was a &lt;a href="https://docs.djangoproject.com/en/1.11/howto/custom-management-commands/"&gt;django-admin command&lt;/a&gt; based on the python &lt;a href="http://docs.python-requests.org/en/master/"&gt;requests&lt;/a&gt; library, that used to parse the downloaded HTML files through the &lt;a href="https://packages.ubuntu.com/xenial/python-lxml"&gt;lxml&lt;/a&gt; library.&lt;/p&gt;
&lt;p&gt;This architecture had at least two inconveniences: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First, since the EC2 instances were not designed to have an high number of requests per second, they were not configured to have a big amount of RAM memory. So, without manually changing the EB configuration, after being activated, the scraping process was killed before reaching complete usage of its available memory by the &lt;a href="http://manpages.ubuntu.com/manpages/xenial/man8/watchdog.8.html"&gt;watchdog&lt;/a&gt; daemon of Ubuntu 16.04. 
In order to solve this problem, I had to manually change the EB configuration, by asking for a different instance type, for instance: t2.medium or t2.large.
This manual process creates a new EC2 instance with enhanced performances, and introduced a downtime period for the back-end. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Second, when the scraper run, slowed down the Django back-end, whom responses took longer time to be generated.
At that time the default configuration on the production environment had the Amazon Auto-scaling feature turned on, that feature, by continuously testing the response times of the instances, starts to create additional EC2 instances if the average response time raises too much. 
That used to happen multiple times during the execution of the Scraping process, since it was manually started on the same EC2 instance, and used to slow down the web server.
Let's naivelly close an eye on the fact that it was inefficient in terms of bills, the problem was that the Auto-scaling system after having increased the number of EC2 instances and having assessed that then the response time lowered, started the process of terminating some of them, in order to decrease their number, since it believes they are not needed anymore.
That termination used to happen in a random fashion, in other words, the terminated EC2 instances were randomly chosen, so, the first EC2 instance risked to be shutdown and to not finish the scraping process. 
As a possible solution, if the Amazon Auto-scaling feature would have been turned off, since on Amazon AWS the HTTPS configuration is strongly tightened with the Auto-scaling one, the production platform turned out to be not reachable via its normal URL, in other words: it was out of service for users.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After some troubles dealing with this architecture, I decided to solve the problem and migrate the scraper module to a different code base that leverages the &lt;a href="https://scrapy.org/"&gt;scrapy&lt;/a&gt; library and &lt;a href="https://scrapinghub.com/"&gt;scrapinghub&lt;/a&gt; platform, but this is another (interesting) story.&lt;/p&gt;
&lt;h4&gt;EDIT:&lt;/h4&gt;
&lt;p&gt;I just get aware that I could have configured autoscaling for &lt;a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/as-instance-termination.html#instance-protection-instance"&gt;instance protection&lt;/a&gt; to avoid the termination of the issued instance where the scraper ran. Well, in the end, by having moved the scraper out of the web server, and having modified the project toward a microservices architecture, in my opinion, still, was not a bad idea.&lt;/p&gt;&lt;/div&gt;</description><category>Amazon AWS</category><category>django</category><category>scrapy</category><guid>http://eracle.github.io/posts/scraper-on-aws-elastic-beanstalk-dont-do-that.html</guid><pubDate>Sat, 09 Sep 2017 07:05:10 GMT</pubDate></item></channel></rss>